# Featur engineering
### 라이브러리, 사용예시
- [Category Encoders](https://contrib.scikit-learn.org/categorical-encoding/) : useful
- [pyod](https://pyod.readthedocs.io/en/latest/) : detection outlying objects

### 참고블로그
- [imputation 정리 블로그](https://data-newbie.tistory.com/257)
- [imbalance 정리 블로그](https://datascienceschool.net/view-notebook/c1a8dad913f74811ae8eef5d3bedc0c3/)

# Interpretation
- [InterpretML ](https://github.com/interpretml/interpret) : open-source python package for training interpretable machine learning models and explaining blackbox systems
- [ALIBI](https://github.com/SeldonIO/alibi/blob/master/README.md) :  open source Python library aimed at machine learning model inspection and interpretation
- [ELI5](https://eli5.readthedocs.io/en/latest/index.html) : Python library which allows to visualize and debug various Machine Learning models using unified API
- [SHAP](https://github.com/slundberg/shap) : game theoretic approach to explain the output of any machine learning model
- [Interpretation 관련 논문](https://github.com/lopusz/awesome-interpretable-machine-learning) : 다있다...!

# Optimization
- [Tutorial on hyperopt](https://www.kaggle.com/fanvacoolt/tutorial-on-hyperopt)

### Boosting hyperparameter
- [Complete Guide to Parameter Tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
- [Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)
- [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)
- [Laurae++ : xgboost/lightgbm](https://sites.google.com/view/lauraepp/parameters) : useful !
- [Xgboost 하이퍼 파라미터 튜닝](https://www.kaggle.com/lifesailor/xgboost) : kaggle 예시
- [Bayes Optimization 기초부터 XGB까지](https://www.kaggle.com/toastls93/bayes-optimization-xgb) : kaggle 예시

# modeling
- [Stacknet](https://github.com/kaz-Anova/StackNet)

# Gaussian process
- [Gpytorch](https://gpytorch.readthedocs.io/en/latest/index.html)

# Pyspark
- [100x-faster-randomized-hyperparameter-searching-framework-with-pyspark](https://towardsdatascience.com/100x-faster-randomized-hyperparameter-searching-framework-with-pyspark-4de19e44f5e6)

# Image
- [cs231n 강의](http://cs231n.stanford.edu/)
- [Image classification 관련 블로그](https://hoya012.github.io/blog/deeplearning-classification-guidebook-1/) : 한글, 설명 잘해줬다.
- [AutoAugment](https://github.com/DeepVoltaire/AutoAugment)
- [fast autoaugment](https://github.com/kakaobrain/fast-autoaugment)

# AutoML
- [auto-sklearn](https://automl.github.io/auto-sklearn/master/index.html#)
- [H2O automl](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)
- [NNI](https://github.com/microsoft/nni) : lightweight but powerful toolkit to help users automate Feature Engineering, Neural Architecture Search, Hyperparameter Tuning and Model Compression
- [automl paper](https://github.com/hibayesian/awesome-automl-papers) : 나중에 공부 참고
- [AutoML-Zero：Evolving Machine Learning Algorithms From Scratch Review](https://hoya012.github.io/blog/automl-zero-review/) : 참고 블로그
- [automl opensource 설명](https://awesomeopensource.com/projects/automl)

# Anomaly detection
- [costcla](http://albahnsen.github.io/CostSensitiveClassification/Intro.html) : cost-sensitive machine learning (classification)
- [논문모음](https://github.com/hoya012/awesome-anomaly-detection)
- [SUALAB](http://research.sualab.com/introduction/review/2020/01/30/anomaly-detection-overview-1.html) : 참고블로그
